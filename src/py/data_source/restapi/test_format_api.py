"""
Test cases for REST API Data Source using PySpark Data Source API

This module tests the proper PySpark Data Source API implementation:
- spark.read.format("restapi").option(...).load()
- spark.write.format("restapi").option(...).save()
- spark.dataSource.register() for registration

These tests use real API calls to JSONPlaceholder API for testing.
"""

import json
import pytest
from pyspark.sql import SparkSession, Row
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
import requests

# Import our REST API Data Source
from restapi import RestApiDataSource


class TestRestApiDataSourceAPI:
    """Test cases for REST API Data Source using PySpark Data Source API"""
    
    @classmethod
    def setup_class(cls):
        """Set up Spark session for testing"""
        cls.spark = SparkSession.builder \
            .appName("REST API Data Source Test") \
            .master("local[*]") \
            .config("spark.sql.adaptive.enabled", "false") \
            .getOrCreate()
        
        # Register our data source
        cls.spark.dataSource.register(RestApiDataSource)
        
        print("âœ… Spark session initialized and REST API data source registered")
    
    @classmethod
    def teardown_class(cls):
        """Clean up Spark session"""
        cls.spark.stop()
        print("ğŸ§¹ Spark session stopped")
    
    def test_read_format_basic(self):
        """Test basic reading using spark.read.format('restapi') with JSONPlaceholder"""
        print("\nğŸ§ª Testing spark.read.format('restapi') - Basic Read")
        
        # Test reading from JSONPlaceholder API (real API call)
        df = self.spark.read \
            .format("restapi") \
            .option("url", "https://jsonplaceholder.typicode.com/users") \
            .option("method", "GET") \
            .load()
        
        # Print the DataFrame
        print("\nğŸ“Š DataFrame content:")
        print(df.show())
        
        # Verify the DataFrame is not None and has data
        assert df is not None
        count = df.count()
        print(f"ğŸ“Š DataFrame count: {count}")
        if count == 0:
            print("âŒ Test failed: DataFrame is empty")
            assert False, "DataFrame should not be empty"
        
        rows = df.collect()
        assert len(rows) == 10  # JSONPlaceholder returns 10 users
        
        # Verify some basic fields exist
        first_row = rows[0]
        assert hasattr(first_row, 'id')
        assert hasattr(first_row, 'name')
        assert hasattr(first_row, 'email')
        
        print(f"âœ… Basic read test passed - Retrieved {len(rows)} users")
    
    def test_read_format_single_user(self):
        """Test reading single user from JSONPlaceholder"""
        print("\nğŸ§ª Testing spark.read.format('restapi') - Single User")
        
        # Test reading single user
        df = self.spark.read \
            .format("restapi") \
            .option("url", "https://jsonplaceholder.typicode.com/users/1") \
            .option("method", "GET") \
            .load()
        
        # Print the DataFrame
        print("\nğŸ“Š Single user DataFrame content:")
        print(df.show())
        
        # Verify single user DataFrame is not None and has data
        assert df is not None
        count = df.count()
        print(f"ğŸ“Š DataFrame count: {count}")
        if count == 0:
            print("âŒ Test failed: DataFrame is empty")
            assert False, "DataFrame should not be empty"
        
        rows = df.collect()
        assert len(rows) == 1
        
        user = rows[0]
        assert user.id == "1"
        assert user.name == "Leanne Graham"
        assert user.email == "Sincere@april.biz"
        
        print("âœ… Single user test passed")
    
    def test_read_format_posts(self):
        """Test reading posts from JSONPlaceholder"""
        print("\nğŸ§ª Testing spark.read.format('restapi') - Posts")
        
        # Test reading posts
        df = self.spark.read \
            .format("restapi") \
            .option("url", "https://jsonplaceholder.typicode.com/posts") \
            .option("method", "GET") \
            .load()
        
        # Print the DataFrame (truncated for readability)
        print("\nğŸ“Š Posts DataFrame content (first 20 rows):")
        print(df.show(20))
        
        # Verify posts DataFrame is not None and has data
        assert df is not None
        count = df.count()
        print(f"ğŸ“Š Posts DataFrame count: {count}")
        if count == 0:
            print("âŒ Test failed: Posts DataFrame is empty")
            assert False, "Posts DataFrame should not be empty"
        
        rows = df.collect()
        assert len(rows) == 100  # JSONPlaceholder returns 100 posts
        
        # Verify some posts have the expected structure
        first_post = rows[0]
        assert hasattr(first_post, 'id')
        assert hasattr(first_post, 'name')  # This will contain the title
        assert first_post.id == "1"
        
        print(f"âœ… Posts test passed - Retrieved {len(rows)} posts")
    
    def test_sql_api_with_temporary_view(self):
        """Test using SQL API with temporary view"""
        print("\nğŸ§ª Testing SQL API with temporary view")
        
        # Create temporary view using our data source
        df = self.spark.read \
            .format("restapi") \
            .option("url", "https://jsonplaceholder.typicode.com/users") \
            .load()
        
        # Print the initial DataFrame
        print("\nğŸ“Š Initial DataFrame content:")
        print(df.show())
        
        # Verify initial DataFrame has data
        count = df.count()
        print(f"ğŸ“Š Initial DataFrame count: {count}")
        if count == 0:
            print("âŒ Test failed: Initial DataFrame is empty")
            assert False, "Initial DataFrame should not be empty"
        
        # Create temporary view
        df.createOrReplaceTempView("users_api")
        
        # Query using SQL
        result = self.spark.sql("SELECT name, email FROM users_api WHERE id = '1'")
        
        # Print the query result
        print("\nğŸ“Š SQL Query result:")
        print(result.show())
        
        # Verify SQL query result has data
        result_count = result.count()
        print(f"ğŸ“Š SQL Query result count: {result_count}")
        if result_count == 0:
            print("âŒ Test failed: SQL Query result is empty")
            assert False, "SQL Query result should not be empty"
        
        rows = result.collect()
        
        # Verify results
        assert len(rows) == 1
        assert rows[0]['name'] == "Leanne Graham"
        assert rows[0]['email'] == "Sincere@april.biz"
        
        print("âœ… SQL API test passed")
    
    def test_schema_inference(self):
        """Test schema inference from API response"""
        print("\nğŸ§ª Testing schema inference")
        
        # Read without explicit schema
        df = self.spark.read \
            .format("restapi") \
            .option("url", "https://jsonplaceholder.typicode.com/users") \
            .load()
        
        # Print the DataFrame with schema
        print("\nğŸ“Š DataFrame content with inferred schema:")
        df.printSchema()
        print(df.show())
        
        # Verify DataFrame has data
        count = df.count()
        print(f"ğŸ“Š Schema inference DataFrame count: {count}")
        if count == 0:
            print("âŒ Test failed: Schema inference DataFrame is empty")
            assert False, "Schema inference DataFrame should not be empty"
        
        # Verify schema fields are present
        field_names = [field.name for field in df.schema.fields]
        assert "id" in field_names
        assert "name" in field_names
        assert "email" in field_names
        
        # Verify data types (all should be strings in our implementation)
        for field in df.schema.fields:
            assert field.dataType == StringType()
        
        print("âœ… Schema inference test passed")
    
    def test_write_format_mock(self):
        """Test write format with mock endpoint (this will fail but tests the API)"""
        print("\nğŸ§ª Testing spark.write.format('restapi') - Mock Write")
        
        # Create test DataFrame
        schema = StructType([
            StructField("title", StringType(), True),
            StructField("body", StringType(), True),
            StructField("userId", IntegerType(), True)
        ])
        
        test_data = [
            ("Test Post 1", "Content 1", 1),
        ]
        df = self.spark.createDataFrame(test_data, schema)
        
        # Try to write (this will fail as expected since we don't have a real write endpoint)
        try:
            df.write \
                .format("restapi") \
                .option("url", "https://jsonplaceholder.typicode.com/posts") \
                .option("method", "POST") \
                .save()
            print("âœ… Write test completed (may have failed as expected)")
        except Exception as e:
            print(f"âœ… Write test completed with expected error: {type(e).__name__}")
    
    def test_error_handling(self):
        """Test error handling with invalid URL"""
        print("\nğŸ§ª Testing error handling")
        
        # Test with invalid URL
        try:
            df = self.spark.read \
                .format("restapi") \
                .option("url", "https://invalid-url-that-does-not-exist.com/users") \
                .load()
            df.collect()  # Trigger the actual API call
            assert False, "Should have raised an exception"
        except Exception as e:
            print(f"âœ… Error handling test passed - Got expected error: {type(e).__name__}")
    
    def test_custom_headers(self):
        """Test with custom headers"""
        print("\nğŸ§ª Testing custom headers")
        
        # Test with custom headers
        custom_headers = {"User-Agent": "RestApiDataSource/1.0"}
        df = self.spark.read \
            .format("restapi") \
            .option("url", "https://jsonplaceholder.typicode.com/users/1") \
            .option("headers", json.dumps(custom_headers)) \
            .load()
        
        # Print the DataFrame
        print("\nğŸ“Š DataFrame content with custom headers:")
        print(df.show())
        
        # Verify the DataFrame is not None and has data
        assert df is not None
        count = df.count()
        print(f"ğŸ“Š Custom headers DataFrame count: {count}")
        if count == 0:
            print("âŒ Test failed: Custom headers DataFrame is empty")
            assert False, "Custom headers DataFrame should not be empty"
        
        rows = df.collect()
        assert len(rows) == 1
        assert rows[0]['name'] == "Leanne Graham"
        
        print("âœ… Custom headers test passed")
    
    def test_streaming_methods_not_implemented(self):
        """Test that streaming methods raise NotImplementedError"""
        print("\nğŸ§ª Testing streaming methods raise NotImplementedError")
        
        from pyspark.sql.types import StructType, StructField, StringType
        from restapi import RestApiDataSource
        
        # Create a data source instance
        data_source = RestApiDataSource()
        data_source.options = {"url": "https://test.com"}
        
        schema = StructType([
            StructField("id", StringType(), True),
            StructField("name", StringType(), True)
        ])
        
        # Test streamReader
        try:
            data_source.streamReader(schema)
            assert False, "streamReader should raise NotImplementedError"
        except NotImplementedError as e:
            assert "Streaming read is not yet implemented" in str(e)
        
        # Test streamWriter
        try:
            data_source.streamWriter(schema, False)
            assert False, "streamWriter should raise NotImplementedError"
        except NotImplementedError as e:
            assert "Streaming write is not yet implemented" in str(e)
        
        # Test simpleStreamReader
        try:
            data_source.simpleStreamReader(schema)
            assert False, "simpleStreamReader should raise NotImplementedError"
        except NotImplementedError as e:
            assert "Simple streaming read is not yet implemented" in str(e)
        
        print("âœ… Streaming methods NotImplementedError test passed")
    
    def test_required_options_validation(self):
        """Test validation of required options"""
        print("\nğŸ§ª Testing required options validation")
        
        from pyspark.sql.types import StructType, StructField, StringType
        from restapi import RestApiDataSource
        
        # Create a data source instance without URL
        data_source = RestApiDataSource()
        data_source.options = {}  # No URL provided
        
        schema = StructType([
            StructField("id", StringType(), True),
            StructField("name", StringType(), True)
        ])
        
        # Test that reader raises ValueError when URL is missing
        try:
            data_source.reader(schema)
            assert False, "reader should raise ValueError when URL is missing"
        except ValueError as e:
            assert "URL option is required" in str(e)
        
        # Test that writer raises ValueError when URL is missing
        try:
            data_source.writer(schema)
            assert False, "writer should raise ValueError when URL is missing"
        except ValueError as e:
            assert "URL option is required" in str(e)
        
        print("âœ… Required options validation test passed")


def run_data_source_tests():
    """Run all DataSource API tests"""
    print("ğŸš€ Starting REST API Data Source Tests")
    print("=" * 80)
    print("This tests the proper PySpark Data Source API implementation using real API calls")
    print("=" * 80)
    
    # Run tests
    test_instance = TestRestApiDataSourceAPI()
    test_instance.setup_class()
    
    try:
        test_instance.test_read_format_basic()
        test_instance.test_read_format_single_user()
        test_instance.test_read_format_posts()
        test_instance.test_sql_api_with_temporary_view()
        test_instance.test_schema_inference()
        test_instance.test_write_format_mock()
        test_instance.test_error_handling()
        test_instance.test_custom_headers()
        test_instance.test_streaming_methods_not_implemented()
        test_instance.test_required_options_validation()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ All DataSource API tests passed!")
        print("âœ… spark.read.format('restapi') works correctly")
        print("âœ… spark.write.format('restapi') API works correctly")
        print("âœ… spark.dataSource.register() works correctly")
        print("âœ… SQL API with temporary views works")
        print("âœ… Schema inference works")
        print("âœ… Error handling works")
        print("âœ… Custom headers work")
        print("âœ… Streaming methods properly raise NotImplementedError")
        print("âœ… Required options validation works")
        
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
    finally:
        test_instance.teardown_class()


if __name__ == "__main__":
    run_data_source_tests() 