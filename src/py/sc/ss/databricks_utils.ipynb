{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîß Databricks Utilities for transformWithState Learning\n",
        "\n",
        "This notebook contains all the utility functions optimized for Databricks environment.\n",
        "Import this notebook to use the functions in other notebooks.\n",
        "\n",
        "## Functions Available:\n",
        "- `create_databricks_spark()` - Optimized Spark session\n",
        "- `create_flight_data()` - Flight data stream generator  \n",
        "- `FlightProcessor` - StatefulProcessor implementation\n",
        "- `run_databricks_demo()` - Complete demo runner\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Spark Session Creation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import max as spark_max, count, expr\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "from pyspark.sql.streaming import StreamingQuery\n",
        "\n",
        "def create_spark() -> SparkSession:\n",
        "    \"\"\"\n",
        "    Create a Spark session optimized for Databricks transformWithState learning.\n",
        "    \n",
        "    Databricks provides full support for transformWithState including:\n",
        "    - Advanced state store providers (RocksDB)\n",
        "    - Reliable checkpointing with DBFS\n",
        "    - Multi-column family state management\n",
        "    - Production-grade streaming infrastructure\n",
        "    \n",
        "    Args:\n",
        "        None\n",
        "        \n",
        "    Returns:\n",
        "        SparkSession: A Databricks-optimized Spark session for transformWithState\n",
        "    \"\"\"\n",
        "    print(\"üîß Creating Databricks Spark session for transformWithState...\")\n",
        "    \n",
        "    # Get or create Spark session (Databricks manages the cluster)\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"LearnTransformWithState_Databricks\") \\\n",
        "        .config(\"spark.sql.streaming.stateStore.providerClass\", \"org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider\") \\\n",
        "        .config(\"spark.sql.streaming.stateStore.rocksdb.formatVersion\", \"5\") \\\n",
        "        .config(\"spark.sql.streaming.stateStore.rocksdb.trackTotalNumberOfRows\", \"true\") \\\n",
        "        .config(\"spark.sql.streaming.stateStore.rocksdb.maxOpenFiles\", \"1000\") \\\n",
        "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "        .config(\"spark.sql.streaming.minBatchesToRetain\", \"2\") \\\n",
        "        .config(\"spark.sql.streaming.stateStore.maintenanceInterval\", \"300s\") \\\n",
        "        .getOrCreate()\n",
        "    \n",
        "    # Set log level for cleaner output\n",
        "    spark.sparkContext.setLogLevel(\"WARN\")\n",
        "    \n",
        "    print(\"‚úÖ Databricks Spark ready with full transformWithState support!\")\n",
        "    print(\"   üóÑÔ∏è  RocksDB state store enabled\")\n",
        "    print(\"   üìÅ DBFS checkpointing available\") \n",
        "    print(\"   üöÄ Production-grade streaming infrastructure\")\n",
        "    return spark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Flight Data Stream Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_flight_data(spark: SparkSession) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Create simple flight data stream.\n",
        "    \n",
        "    Args:\n",
        "        spark: The Spark session to use for creating the stream\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame: A streaming DataFrame with flight state updates\n",
        "    \"\"\"\n",
        "    print(\"üìä Creating flight data stream...\")\n",
        "    \n",
        "    # Use rate source to generate data\n",
        "    rate_stream = spark \\\n",
        "        .readStream \\\n",
        "        .format(\"rate\") \\\n",
        "        .option(\"rowsPerSecond\", 1) \\\n",
        "        .load()\n",
        "    \n",
        "    # Transform to simple flight data\n",
        "    flight_stream = rate_stream.selectExpr(\n",
        "        # Three flights cycling - realistic flight numbers\n",
        "        \"\"\"CASE (value % 3)\n",
        "            WHEN 0 THEN 'Delta1247'\n",
        "            WHEN 1 THEN 'United892'\n",
        "            ELSE 'Southwest5031'\n",
        "        END as flight\"\"\",\n",
        "        \n",
        "        # Simple state progression: boarding -> flying -> landed\n",
        "        \"\"\"CASE (value % 3)\n",
        "            WHEN 0 THEN 'boarding'\n",
        "            WHEN 1 THEN 'flying'\n",
        "            ELSE 'landed'\n",
        "        END as state\"\"\",\n",
        "        \n",
        "        \"timestamp\"\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Flight data ready!\")\n",
        "    return flight_stream\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è FlightProcessor - StatefulProcessor Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Iterator, Any, Dict, List\n",
        "import pandas as pd\n",
        "from pyspark.sql.streaming import StatefulProcessor, StatefulProcessorHandle\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "class FlightProcessor(StatefulProcessor):\n",
        "    \"\"\"\n",
        "    The simplest flight state processor for learning transformWithState on Databricks.\n",
        "    \n",
        "    Powered by Databricks' production-grade infrastructure:\n",
        "    - RocksDB state store with multi-column family support\n",
        "    - DBFS checkpointing for fault tolerance  \n",
        "    - Auto-scaling clusters for performance\n",
        "    \n",
        "    Tracks flights through 3 states: boarding -> flying -> landed\n",
        "    \"\"\"\n",
        "    \n",
        "    def init(self, handle: StatefulProcessorHandle) -> None:\n",
        "        \"\"\"\n",
        "        Set up the processor when it starts.\n",
        "        \n",
        "        Args:\n",
        "            handle: Spark's handle for managing state and timers\n",
        "            \n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        # Define what we store for each flight\n",
        "        state_schema = StructType([\n",
        "            StructField(\"flight\", StringType(), True),      # Flight name like \"Delta1247\"\n",
        "            StructField(\"state\", StringType(), True),       # Current state\n",
        "            StructField(\"count\", IntegerType(), True)       # How many updates\n",
        "        ])\n",
        "        \n",
        "        # Create state storage - RocksDB handles multiple column families!\n",
        "        self.flight_state = handle.getValueState(\"flight_state\", state_schema)\n",
        "    \n",
        "    def handleInputRows(self, key: str, rows: Iterator[pd.DataFrame], timerValues: Any) -> Iterator[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Process flight updates for one flight.\n",
        "        \n",
        "        Args:\n",
        "            key: The flight identifier (e.g., \"Delta1247\")\n",
        "            rows: Iterator of DataFrames containing flight state updates\n",
        "            timerValues: Timer information (not used in this example)\n",
        "            \n",
        "        Returns:\n",
        "            Iterator[pd.DataFrame]: Iterator containing result DataFrame with flight state info\n",
        "        \"\"\"\n",
        "        \n",
        "        # Get all updates for this flight\n",
        "        all_rows = []\n",
        "        for batch in rows:\n",
        "            all_rows.append(batch)\n",
        "        \n",
        "        if not all_rows:\n",
        "            return iter([])\n",
        "        \n",
        "        # Combine all updates\n",
        "        updates = pd.concat(all_rows, ignore_index=True)\n",
        "        \n",
        "        # Get current state for this flight\n",
        "        # Databricks RocksDB handles this efficiently with multi-column family support!\n",
        "        if self.flight_state.exists():\n",
        "            current = self.flight_state.get().iloc[0]\n",
        "            current_state = current['state']\n",
        "            current_count = current['count']\n",
        "            # Optional: Add logging to see Databricks state management in action\n",
        "            if current_count % 5 == 0:  # Log every 5th update\n",
        "                print(f\"üóÑÔ∏è  Databricks RocksDB: {key} state retrieved (count: {current_count})\")\n",
        "        else:\n",
        "            current_state = \"unknown\"\n",
        "            current_count = 0\n",
        "            print(f\"üÜï New flight {key} starting in Databricks state store\")\n",
        "        \n",
        "        # Process each update\n",
        "        for _, update in updates.iterrows():\n",
        "            new_state = update['state']\n",
        "            flight_name = update['flight']\n",
        "            \n",
        "            # Simple validation: only allow valid progressions\n",
        "            if self._is_valid_transition(current_state, new_state):\n",
        "                current_state = new_state\n",
        "                current_count += 1\n",
        "                # Optional: Show state transitions\n",
        "                if current_count <= 3:  # Show first few transitions\n",
        "                    print(f\"‚úÖ {flight_name}: {update['state']} -> {current_state} (update #{current_count})\")\n",
        "        \n",
        "        # Save the new state to Databricks RocksDB\n",
        "        new_state_data = pd.DataFrame({\n",
        "            'flight': [flight_name],\n",
        "            'state': [current_state],\n",
        "            'count': [current_count]\n",
        "        })\n",
        "        self.flight_state.update(new_state_data)\n",
        "        \n",
        "        # Return the result\n",
        "        result = pd.DataFrame({\n",
        "            'flight': [flight_name],\n",
        "            'current_state': [current_state],\n",
        "            'update_count': [str(current_count)]\n",
        "        })\n",
        "        \n",
        "        return iter([result])\n",
        "    \n",
        "    def _is_valid_transition(self, from_state: str, to_state: str) -> bool:\n",
        "        \"\"\"\n",
        "        Check if state transition is allowed.\n",
        "        \n",
        "        Args:\n",
        "            from_state: Current state of the flight\n",
        "            to_state: Desired new state for the flight\n",
        "            \n",
        "        Returns:\n",
        "            bool: True if transition is valid, False otherwise\n",
        "        \"\"\"\n",
        "        # Simple rules: boarding -> flying -> landed\n",
        "        valid_moves = {\n",
        "            \"unknown\": [\"boarding\"],           # New flights start boarding\n",
        "            \"boarding\": [\"flying\"],            # From boarding, can fly\n",
        "            \"flying\": [\"landed\"],              # From flying, can land\n",
        "            \"landed\": [\"boarding\"]             # From landed, can start new journey\n",
        "        }\n",
        "        \n",
        "        allowed = valid_moves.get(from_state, [])\n",
        "        return to_state in allowed\n",
        "    \n",
        "    def handleExpiredTimer(self, key: str, timerValues: Any, expiredTimerInfo: Any) -> Iterator[pd.DataFrame]:\n",
        "        \"\"\"\n",
        "        Handle timers (not used in this simple example).\n",
        "        \n",
        "        Args:\n",
        "            key: The flight identifier for which the timer expired\n",
        "            timerValues: Timer values\n",
        "            expiredTimerInfo: Information about the expired timer\n",
        "            \n",
        "        Returns:\n",
        "            Iterator[pd.DataFrame]: Empty iterator since timers are not used\n",
        "        \"\"\"\n",
        "        return iter([])\n",
        "    \n",
        "    def close(self) -> None:\n",
        "        \"\"\"\n",
        "        Clean up when done.\n",
        "        \n",
        "        Args:\n",
        "            None\n",
        "            \n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Demo Runner Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "def run_learning_demo(spark: SparkSession) -> None:\n",
        "    \"\"\"\n",
        "    Run the transformWithState demo on Databricks for learning stateful processing.\n",
        "    \n",
        "    This uses the actual transformWithState API with full Databricks support:\n",
        "    - RocksDB state store for reliable state management\n",
        "    - DBFS checkpointing for fault tolerance\n",
        "    - Production-grade streaming infrastructure\n",
        "    \n",
        "    Args:\n",
        "        spark: The Databricks Spark session to use for the demo\n",
        "        \n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"üéì\" + \"=\"*50)\n",
        "    print(\"LEARNING DEMO: transformWithState on Databricks\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    # Create flight data\n",
        "    flight_data = create_flight_data(spark)\n",
        "    \n",
        "    # Define output schema\n",
        "    output_schema = StructType([\n",
        "        StructField(\"flight\", StringType(), True),\n",
        "        StructField(\"current_state\", StringType(), True),\n",
        "        StructField(\"update_count\", StringType(), True)\n",
        "    ])\n",
        "    \n",
        "    # Apply transformWithState - this is the key learning point!\n",
        "    # Databricks fully supports this with RocksDB state store\n",
        "    flight_states = flight_data \\\n",
        "        .groupBy(\"flight\") \\\n",
        "        .transformWithStateInPandas(\n",
        "            statefulProcessor=FlightProcessor(),\n",
        "            outputStructType=output_schema,\n",
        "            outputMode=\"Update\",\n",
        "            timeMode=\"ProcessingTime\"\n",
        "        )\n",
        "    \n",
        "    # Use DBFS for reliable checkpointing in Databricks\n",
        "    checkpoint_dir = f\"/tmp/learn_checkpoint_{uuid.uuid4().hex[:8]}\"\n",
        "    \n",
        "    print(f\"üìÅ Using checkpoint location: {checkpoint_dir}\")\n",
        "    \n",
        "    # Start the stream with Databricks-optimized settings\n",
        "    query = flight_states \\\n",
        "        .writeStream \\\n",
        "        .outputMode(\"update\") \\\n",
        "        .format(\"console\") \\\n",
        "        .option(\"truncate\", False) \\\n",
        "        .option(\"numRows\", 15) \\\n",
        "        .option(\"checkpointLocation\", checkpoint_dir) \\\n",
        "        .trigger(processingTime='5 seconds') \\\n",
        "        .start()\n",
        "    \n",
        "    print(\"\\nüöÄ Demo running on Databricks! Watch transformWithState in action...\")\n",
        "    print(\"üìù Key things to notice:\")\n",
        "    print(\"   - üóÑÔ∏è  RocksDB manages state reliably for each flight\")  \n",
        "    print(\"   - ‚úÖ State transitions are validated in real-time\")\n",
        "    print(\"   - üìà Update counts increase over time\")\n",
        "    print(\"   - üíæ State persists across batches with checkpointing\")\n",
        "    print(\"   - üöÄ This is production-grade transformWithState!\")\n",
        "    print(f\"   - üìÅ Checkpoint: {checkpoint_dir}\")\n",
        "    print(\"\\n‚èπÔ∏è  Press Ctrl+C to stop when you've learned enough!\")\n",
        "    \n",
        "    try:\n",
        "        query.awaitTermination()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüõë Stopping demo...\")\n",
        "        query.stop()\n",
        "        print(\"‚úÖ Demo complete! You've mastered transformWithState on Databricks.\")\n",
        "        print(f\"üìÅ Checkpoint preserved at: {checkpoint_dir}\")\n",
        "    finally:\n",
        "        # Ensure clean shutdown\n",
        "        if query.isActive:\n",
        "            query.stop()\n",
        "            print(\"üîÑ Stream stopped gracefully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Concept Explanation Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def explain_basics() -> None:\n",
        "    \"\"\"\n",
        "    Explain the core concepts simply.\n",
        "    \n",
        "    Args:\n",
        "        None\n",
        "        \n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"üìö\" + \"=\"*60)\n",
        "    print(\"TRANSFORM WITH STATE ON DATABRICKS\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"\"\"\n",
        "üéØ THE BIG IDEA:\n",
        "   Keep information about each thing (like flights) between batches\n",
        "\n",
        "üîë KEY CONCEPTS:\n",
        "\n",
        "1. GROUPING BY KEY\n",
        "   .groupBy(\"flight\")  ‚Üê Each flight gets separate processing\n",
        "\n",
        "2. STATE STORAGE  \n",
        "   Each flight remembers its current state (boarding/flying/landed)\n",
        "\n",
        "3. BATCH PROCESSING\n",
        "   Every few seconds, process new updates for each flight\n",
        "\n",
        "4. STATE PERSISTENCE\n",
        "   Flight state survives between batches - that's the magic!\n",
        "\n",
        "üõ´ OUR EXAMPLE:\n",
        "   - Track flights: Delta1247, United892, Southwest5031\n",
        "   - States: boarding ‚Üí flying ‚Üí landed\n",
        "   - Each flight remembers where it is\n",
        "\n",
        "üß† MENTAL MODEL:\n",
        "   Think of it like having a notebook for each flight.\n",
        "   Every batch, you:\n",
        "   1. Look up the flight's current page in the notebook\n",
        "   2. Read what state it was in\n",
        "   3. Update it based on new information  \n",
        "   4. Write the new state back to the notebook\n",
        "   5. The notebook persists for the next batch!\n",
        "\n",
        "üèóÔ∏è DATABRICKS ADVANTAGES:\n",
        "   - üóÑÔ∏è  RocksDB state store (production-grade)\n",
        "   - üìÅ DBFS checkpointing (fault tolerance)\n",
        "   - üöÄ Auto-scaling clusters (performance)\n",
        "   - üíæ Multi-column family support (advanced features)\n",
        "   - üîß Managed infrastructure (no setup headaches)\n",
        "\n",
        "‚öôÔ∏è THE API:\n",
        "   - transformWithState gives you full control\n",
        "   - StatefulProcessor handles the state logic\n",
        "   - You decide what to store and how to update it\n",
        "   - Databricks makes it production-ready!\n",
        "\"\"\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"üöÄ READY TO SEE IT ON DATABRICKS!\")\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ All Functions Ready!\n",
        "\n",
        "You can now use these functions in other notebooks:\n",
        "\n",
        "```python\n",
        "# Import this notebook\n",
        "%run ./databricks_utils\n",
        "\n",
        "# Create Spark session\n",
        "spark = create_spark()\n",
        "\n",
        "# Run the demo\n",
        "run_learning_demo(spark)\n",
        "\n",
        "# Or explain concepts first\n",
        "explain_basics()\n",
        "```\n",
        "\n",
        "**Available Functions:**\n",
        "- `create_spark()` - Databricks-optimized Spark session\n",
        "- `create_flight_data(spark)` - Flight data stream generator\n",
        "- `FlightProcessor` - StatefulProcessor class for transformWithState\n",
        "- `run_learning_demo(spark)` - Complete interactive demo\n",
        "- `explain_basics()` - Concept explanation\n",
        "\n",
        "**Production Features:**\n",
        "- üóÑÔ∏è RocksDB state store with multi-column family support\n",
        "- üìÅ DBFS checkpointing for fault tolerance\n",
        "- üöÄ Auto-scaling clusters for performance\n",
        "- üíæ Managed infrastructure (zero setup)\n",
        "- üîß Production-grade streaming infrastructure\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
